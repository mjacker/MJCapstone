{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# 2. Dataset Preparation\n",
    "---\n",
    "For this Capstone, are eelected to be procesed two files from #[CSE-CIC-IDS2018](https://www.unb.ca/cic/datasets/ids-2018.html) those are \n",
    "- `Friday-16-02-2018_TrafficForML_CICFlowMeter.csv`\n",
    "This file contains most of Dos attacks\n",
    "\n",
    "- `Friday-02-03-2018_TrafficForML_CICFlowMeter.csv`\n",
    "This file contains most of botnet computers.\n",
    "\n",
    "since these two files contains a large malicius packages, it will help help to balance the dataset which will be uses to train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading path to dataset files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FILES_PATH = []\n",
    "for path, _, file in (os.walk(\"./datasets/\")):\n",
    "    for eachFile in file:\n",
    "        DATASET_FILES_PATH.append(path + eachFile)\n",
    "DATASET_FILES_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion / versioning\n",
    "### Concatenating datasetsLoading datasets to PandaData Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# df_dataset = pd.read_csv(DATASET_FILES_PATH[0])\n",
    "# print(df_dataset.shape)\n",
    "\n",
    "# For Google Colab, due to memory capacity, only can handle one day dataset.\n",
    "df_friday1 = pd.read_csv(DATASET_FILES_PATH[0])\n",
    "df_friday2 = pd.read_csv(DATASET_FILES_PATH[1])\n",
    "# # For Google Colab, due to memory capacity, only can handle one day dataset.\n",
    "df_dataset = pd.concat([df_friday1, df_friday2], axis=0, ignore_index=True)\n",
    "# # Because two datasets was concatenated, then need to delete the row which cointain the second dataframe title\n",
    "df_dataset.drop(df_dataset.loc[df_dataset[\"Label\"] == \"Label\"].index, inplace=True)\n",
    "print(df_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### TEMP\n",
    "\n",
    "# # para agilizar y progar\n",
    "\n",
    "# # Drop rows from index 30 to 100\n",
    "# df_dataset = df_dataset.drop(index=range(200000, 1048575))\n",
    "# print(df_dataset.shape)\n",
    "\n",
    "\n",
    "##TEMMPPP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "- ### Drop unrelated columns\n",
    "Since Port, protocol and the timestand are not related to the label with those selectec machine learning, those will be droped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.drop(columns=['Dst Port', 'Protocol', 'Timestamp'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Droping rows with infinite or null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape before deleting rows: \", df_dataset.shape)\n",
    "df_dataset[df_dataset.isnull().any(axis=1)]\n",
    "df_dataset.replace([np.inf, -np.inf], np.nan)\n",
    "df_dataset.dropna(inplace=True)\n",
    "print(\"Shape after deteling rows:\", df_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "### Check Label labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_dataset['Label'].unique())\n",
    "print(df_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Changing Labels names \n",
    "To unify the labels, those malicius packages will be renamend as ones, and the normal as zeros.\n",
    "- 0 - normal package\n",
    "- 1 - malicius package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_dataset.replace(to_replace=['Benign'], value=0, inplace=True)\n",
    "df_dataset.replace(to_replace=[\"Bot\", \"DoS attacks-SlowHTTPTest\", \"DoS attacks-Hulk\"], value=1, inplace=True)\n",
    "df_dataset[df_dataset.columns[-1]].unique()\n",
    "# some values are saved as string, but actually they should be integer values, forcing here changing types\n",
    "df_dataset.astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_dataset.shape)\n",
    "df_dataset.drop_duplicates(inplace=True)\n",
    "print(df_dataset.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check columns datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions labels after drop rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_benign = df_dataset[\"Label\"].value_counts()[[0]].sum()\n",
    "label_malicious = df_dataset[\"Label\"].value_counts()[[1]].sum()\n",
    "\n",
    "print(df_dataset.shape)\n",
    "\n",
    "abs_values = [label_benign, label_malicious]\n",
    "sns.set(rc={'figure.figsize':(8, 6)})\n",
    "ax = sns.countplot(x=df_dataset[df_dataset.columns[-1]], \n",
    "              data = df_dataset,\n",
    "              palette = 'dark:#5A9_r')\n",
    "ax.bar_label(container=ax.containers[0], labels=[label_benign])\n",
    "# ax.bar_label(container=ax.containers[1], labels=[label_malicious]) # fails compile in google colab\n",
    "plt.xlabel(f\"0 = Bening; 1 = Malicious\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inbalance problem\n",
    "There are two ways to solve this problem, \n",
    "1. Droping bening rows.\n",
    "2. Sampling Malicious rows.\n",
    "\n",
    "for this attempt, I am dropping bening rows until it gets balanced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.drop(df_dataset[df_dataset.Label == 0].index[-(label_benign-label_malicious):], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions labels after drop fixing imbalance problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_benign = df_dataset[\"Label\"].value_counts()[[0]].sum()\n",
    "label_malicious = df_dataset[\"Label\"].value_counts()[[1]].sum()\n",
    "\n",
    "print(df_dataset.shape)\n",
    "\n",
    "abs_values = [label_benign, label_malicious]\n",
    "sns.set(rc={'figure.figsize':(8, 6)})\n",
    "ax = sns.countplot(x=df_dataset[df_dataset.columns[-1]], \n",
    "              data = df_dataset,\n",
    "              palette = 'dark:#5A9_r')\n",
    "ax.bar_label(container=ax.containers[0], labels=[label_benign])\n",
    "# ax.bar_label(container=ax.containers[1], labels=[label_malicious]) # fails compile on google colab\n",
    "plt.xlabel(f\"0 = Bening; 1 = Malicious\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Dataset as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.to_csv(\"processed_dataset_in_2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
