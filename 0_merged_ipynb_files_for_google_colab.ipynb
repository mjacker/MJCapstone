{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setting up enviroments requirements\n",
    "If you want to run this Jupyter Notebook on Google colab, clieck on the next hyperlink: [Load on Google Colab.](https://githubtocolab.com/mjacker/MJCapstone/blob/master/0_merged_ipynb_files_for_google_colab.ipynb)\n",
    "\n",
    "If you want to load the Jypyter Notebook locally in your computer then clone the github repository on [Github Repository](https://github.com/mjacker/MJCapstone/tree/develop) installing the requirements from the `requirements.yml` file with `# python -m pip install -r requirements.yml`.\n",
    "\n",
    "Uncomment the next block to install dependencies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the Dataset\n",
    "\n",
    "### Downloading on Google colab. (by default)\n",
    "\n",
    "Since google colab is running on linux, most depencencies are already installed in it, but in order to download the dataset from amazon web services first needs to install the aws-cli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tested on linux (Google-Colab)\n",
    "!apt-get install awscli\n",
    "!python -m pip install requests==2.28.2\n",
    "!mkdir datasets\n",
    "!aws s3 ls --no-sign-request --region ap-northeast-3 \"s3://cse-cic-ids2018/\" --recursive --human-readable\n",
    "!aws s3 cp --no-sign-request --region ap-northeast-3 \"s3://cse-cic-ids2018/Processed Traffic Data for ML Algorithms/Friday-02-03-2018_TrafficForML_CICFlowMeter.csv\" \"./datasets/\"\n",
    "!aws s3 cp --no-sign-request --region ap-northeast-3 \"s3://cse-cic-ids2018/Processed Traffic Data for ML Algorithms/Friday-16-02-2018_TrafficForML_CICFlowMeter.csv\" \"./datasets/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading on windows.\n",
    "\n",
    "Trying Downloading on windows I had realize that this could be achieve with a different aproach, using boto3 library from python. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tested on windows 10\n",
    "# On powershell 7.4\n",
    "\n",
    "# # !python -m pip install boto3\n",
    "# !python .\\scripts\\download-cic-ids-dataset.py \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# 2. Dataset Preparation\n",
    "---\n",
    "For this Capstone, are eelected to be procesed two files from #[CSE-CIC-IDS2018](https://www.unb.ca/cic/datasets/ids-2018.html) those are \n",
    "- `Friday-16-02-2018_TrafficForML_CICFlowMeter.csv`\n",
    "This file contains most of Dos attacks\n",
    "\n",
    "- `Friday-02-03-2018_TrafficForML_CICFlowMeter.csv`\n",
    "This file contains most of botnet computers.\n",
    "\n",
    "since these two files contains a large malicius packages, it will help help to balance the dataset which will be uses to train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading path to dataset files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FILES_PATH = []\n",
    "for path, _, file in (os.walk(\"./datasets/\")):\n",
    "    for eachFile in file:\n",
    "        DATASET_FILES_PATH.append(path + eachFile)\n",
    "DATASET_FILES_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion / versioning\n",
    "### Concatenating datasetsLoading datasets to PandaData Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# df_dataset = pd.read_csv(DATASET_FILES_PATH[0])\n",
    "# print(df_dataset.shape)\n",
    "\n",
    "# For Google Colab, due to memory capacity, only can handle one day dataset.\n",
    "df_friday1 = pd.read_csv(DATASET_FILES_PATH[0])\n",
    "df_friday2 = pd.read_csv(DATASET_FILES_PATH[1])\n",
    "# # For Google Colab, due to memory capacity, only can handle one day dataset.\n",
    "df_dataset = pd.concat([df_friday1, df_friday2], axis=0, ignore_index=True)\n",
    "# # Because two datasets was concatenated, then need to delete the row which cointain the second dataframe title\n",
    "df_dataset.drop(df_dataset.loc[df_dataset[\"Label\"] == \"Label\"].index, inplace=True)\n",
    "print(df_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "- ### Drop unrelated columns\n",
    "Since Port, protocol and the timestand are not related to the label with those selectec machine learning, those will be droped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.drop(columns=['Dst Port', 'Protocol', 'Timestamp'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Droping rows with infinite or null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape before deleting rows: \", df_dataset.shape)\n",
    "df_dataset[df_dataset.isnull().any(axis=1)]\n",
    "df_dataset.replace([np.inf, -np.inf], np.nan)\n",
    "df_dataset.dropna(inplace=True)\n",
    "print(\"Shape after deteling rows:\", df_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "### Check Label labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_dataset['Label'].unique())\n",
    "print(df_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Changing Labels names \n",
    "To unify the labels, those malicius packages will be renamend as ones, and the normal as zeros.\n",
    "- 0 - normal package\n",
    "- 1 - malicius package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_dataset.replace(to_replace=['Benign'], value=0, inplace=True)\n",
    "df_dataset.replace(to_replace=[\"Bot\", \"DoS attacks-SlowHTTPTest\", \"DoS attacks-Hulk\"], value=1, inplace=True)\n",
    "df_dataset[df_dataset.columns[-1]].unique()\n",
    "# some values are saved as string, but actually they should be integer values, forcing here changing types\n",
    "df_dataset.astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_dataset.shape)\n",
    "df_dataset.drop_duplicates(inplace=True)\n",
    "print(df_dataset.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check columns datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions labels after drop rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_benign = df_dataset[\"Label\"].value_counts()[[0]].sum()\n",
    "label_malicious = df_dataset[\"Label\"].value_counts()[[1]].sum()\n",
    "\n",
    "print(df_dataset.shape)\n",
    "\n",
    "abs_values = [label_benign, label_malicious]\n",
    "sns.set(rc={'figure.figsize':(8, 6)})\n",
    "ax = sns.countplot(x=df_dataset[df_dataset.columns[-1]], \n",
    "              data = df_dataset,\n",
    "              palette = 'dark:#5A9_r')\n",
    "ax.bar_label(container=ax.containers[0], labels=[label_benign])\n",
    "ax.bar_label(container=ax.containers[1], labels=[label_malicious])\n",
    "plt.xlabel(f\"0 = Bening; 1 = Malicious\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inbalance problem\n",
    "There are two ways to solve this problem, \n",
    "1. Droping bening rows.\n",
    "2. Sampling Malicious rows.\n",
    "\n",
    "for this attempt, I am dropping bening rows until it gets balanced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.drop(df_dataset[df_dataset.Label == 0].index[-(label_benign-label_malicious):], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions labels after drop fixing imbalance problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_benign = df_dataset[\"Label\"].value_counts()[[0]].sum()\n",
    "label_malicious = df_dataset[\"Label\"].value_counts()[[1]].sum()\n",
    "\n",
    "print(df_dataset.shape)\n",
    "\n",
    "abs_values = [label_benign, label_malicious]\n",
    "sns.set(rc={'figure.figsize':(8, 6)})\n",
    "ax = sns.countplot(x=df_dataset[df_dataset.columns[-1]], \n",
    "              data = df_dataset,\n",
    "              palette = 'dark:#5A9_r')\n",
    "ax.bar_label(container=ax.containers[0], labels=[label_benign])\n",
    "ax.bar_label(container=ax.containers[1], labels=[label_malicious])\n",
    "plt.xlabel(f\"0 = Bening; 1 = Malicious\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Dataset as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.to_csv(\"processed_dataset_in_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploratory - Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading processed dataset to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = pd.read_csv(\"processed_dataset_in_2.csv\")\n",
    "df_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson correlation between the input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEMP\n",
    "df_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "corr_matrix = df_dataset.corr()\n",
    "plt.rcParams['figure.figsize'] = (16, 9)\n",
    "g = sns.heatmap(corr_matrix, \n",
    "                cmap=\"coolwarm\", #colos\n",
    "                annot=False, # add a value to each cell \n",
    "                fmt='.1g',\n",
    "                vmin = -1, \n",
    "                vmax = 1)\n",
    "g.set_xticklabels(g.get_xticklabels(), verticalalignment='top', horizontalalignment='right', rotation=30);\n",
    "plt.savefig('corr_heatmap1.png', dpi=800, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dropping not valuable columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = []\n",
    "\n",
    "for x, e in enumerate(df_dataset.columns[:-1]):\n",
    "    if (len(df_dataset[df_dataset.columns[x]].unique()) == 1):\n",
    "        print(x, e, df_dataset[df_dataset.columns[x]].unique())\n",
    "        columns_to_drop.append(x)\n",
    "\n",
    "# columns_to_drop\n",
    "# for x in columns_to_drop:\n",
    "#     df_dataset.drop(columns=[df_dataset.columns[x]], axis=1, inplace = True)\n",
    "\n",
    "print(\"Columns to drop: \", columns_to_drop)\n",
    "print(\"Dataframe shape: \", df_dataset.shape)\n",
    "\n",
    "reversed_order = list(reversed(columns_to_drop))\n",
    "print(reversed_order)\n",
    "for x in reversed_order:\n",
    "    df_dataset.drop(columns=[df_dataset.columns[x]], inplace = True)\n",
    "\n",
    "df_dataset.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking pearson correlation after drops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df_dataset.corr()\n",
    "plt.rcParams['figure.figsize'] = (16, 9)\n",
    "g = sns.heatmap(corr_matrix, \n",
    "                cmap=\"coolwarm\", #colos\n",
    "                annot=False, # add a value to each cell \n",
    "                fmt='.2f',\n",
    "                vmin = -1, \n",
    "                vmax = 1)\n",
    "g.set_xticklabels(g.get_xticklabels(), verticalalignment='top', horizontalalignment='right', rotation=30);\n",
    "plt.savefig('corr_heatmap2.png', dpi=800, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dataset.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "# df_dataset.dropna(inplace=True)\n",
    "\n",
    "# print(np.any(np.isnan(df_dataset)))\n",
    "# print(np.any(np.isinf(df_dataset)))\n",
    "\n",
    "# # si trato de usar where infinite, normalmente trae malos resultados onda overflow de memoria\n",
    "# df_dataset.isin([np.inf, -np.inf]).values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# y = np.array(df_dataset.pop('Label'))\n",
    "# X = np.array(df_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# X_scaler = MinMaxScaler().fit(X)\n",
    "# pd.DataFrame(X_scaler.transform(X))\n",
    "# X = np.array(X_scaler.transform(X))\n",
    "# X\n",
    "# df_dataset = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.to_csv(\"processed_dataset_in_3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "\n",
    "# For reproducible results\n",
    "RANDOM_STATE_SEED = 732"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = pd.read_csv(\"processed_dataset_in_3.csv\")\n",
    "df_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# es realmente necesario volver a filtrar los datos si supuestamente el procesado no deveria tener valores infinitos\n",
    "\n",
    "print(np.any(np.isnan(df_dataset)))\n",
    "print(np.any(np.isinf(df_dataset)))\n",
    "\n",
    "# si trato de usar where infinite, normalmente trae malos resultados onda overflow de memoria\n",
    "df_dataset.isin([np.inf, -np.inf]).values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dataset.isinf()\n",
    "df_dataset.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_dataset.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# es realmente necesario volver a filtrar los datos si supuestamente el procesado no deveria tener valores infinitos\n",
    "\n",
    "print(np.any(np.isnan(df_dataset)))\n",
    "print(np.any(np.isinf(df_dataset)))\n",
    "\n",
    "# si trato de usar where infinite, normalmente trae malos resultados onda overflow de memoria\n",
    "df_dataset.isin([np.inf, -np.inf]).values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(df_dataset.pop('Label'))\n",
    "X = np.array(df_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X_scaler = MinMaxScaler().fit(X)\n",
    "pd.DataFrame(X_scaler.transform(X))\n",
    "X = np.array(X_scaler.transform(X))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = train_test_split(df_dataset, test_size=0.3, random_state=RANDOM_STATE_SEED)\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=RANDOM_STATE_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight  # For balanced class weighted classification training\n",
    "\n",
    "# Calculating class weights for balanced class weighted classifier training\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "print(class_weights)\n",
    "\n",
    "# Must be in dict format for scikitlearn\n",
    "class_weights = {\n",
    "    0: class_weights[0],\n",
    "    1: class_weights[1]\n",
    "}\n",
    "\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# predictions\n",
    "# joblib.dump(model, r\".\\trained_models\\remote-random-forest-classifier.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = joblib.load(f\".\\trained_models\\remote-random-forest-classifier\")\n",
    "# model = joblib.load(r\".\\trained_models\\remote-random-forest-classifier.pkl\")\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Step 7: Comparing Decision Tree, Random Forest, XGBoost, CatBoost, and LightGBM\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Bagging' : BaggingClassifier(),\n",
    "    'XGBoost': XGBClassifier(),\n",
    "    'CatBoost': CatBoostClassifier(),\n",
    "    'LightGBM': LGBMClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEMP\n",
    "classifiers.items()\n",
    "# for name, clf in classifiers.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate classifiers\n",
    "results = {}\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    results[name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Confusion Matrix': confusion_mat,\n",
    "        'Classification Report': class_report,\n",
    "        'ROC Curve': (fpr, tpr, roc_auc)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot for accuracy comparison\n",
    "accuracy_values = [result['Accuracy'] for result in results.values()]\n",
    "classifiers_names = list(classifiers.keys())\n",
    "\n",
    "plt.figure(figsize=(7, 3))\n",
    "plt.bar(classifiers_names, accuracy_values, color=['blue', 'green', 'red', 'purple', 'orange'])\n",
    "plt.xlabel('Classifiers')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Classifier Accuracy Comparison')\n",
    "plt.ylim([0, 1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices and classification reports\n",
    "for name, result in results.items():\n",
    "    print(f'\\n{name}:\\n')\n",
    "    # print(f'Confusion Matrix:\\n{result[\"Confusion Matrix\"]}\\n')\n",
    "    print(f'Classification Report:\\n{result[\"Classification Report\"]}\\n')\n",
    "\n",
    "    # Plot Confusion Matrix with Blues Colormap\n",
    "    plt.figure(figsize=(4, 2))\n",
    "    sns.heatmap(result[\"Confusion Matrix\"], annot=True, fmt='g', cmap=plt.cm.Greens, cbar=False)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix - {name}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Plot ROC curves\n",
    "plt.figure(figsize=(20, 6))\n",
    "for name, result in results.items():\n",
    "    fpr, tpr, roc_auc = result['ROC Curve']\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.9f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_report = classification_report(true,\n",
    "                                   pred,\n",
    "                                   labels=labels,\n",
    "                                   target_names=target_names,\n",
    "                                   output_dict=True)\n",
    "sns.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Model\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "\n",
    "# For reproducible results\n",
    "RANDOM_STATE_SEED = 420"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = pd.read_csv(\"processed_dataset_in_3.csv\")\n",
    "df_dataset\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# es realmente necesario volver a filtrar los datos si supuestamente el procesado no deveria tener valores infinitos\n",
    "\n",
    "print(np.any(np.isnan(df_dataset)))\n",
    "print(np.any(np.isinf(df_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_dataset.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.any(np.isnan(df_dataset)))\n",
    "print(np.any(np.isinf(df_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(df_dataset.pop('Label'))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df_dataset)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_dataset.shape)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pato = pd.DataFrame(X)\n",
    "pato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pienso que aqui tengo que agregar uso de baja los valores\n",
    "# no, tengo que hacer despues de la separacion X e Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMP\n",
    "len(df_dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_X.shape)\n",
    "# print(df_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test = train_test_split(df_dataset, test_size=0.3, random_state=RANDOM_STATE_SEED)\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=RANDOM_STATE_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_dataset.shape)\n",
    "\n",
    "print(\"TRAIN:\")\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(\"TEST\")\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(\n",
    "    criterion='gini',\n",
    "    splitter='best',\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features=None,\n",
    "    random_state=None,\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    class_weight=None,\n",
    "    ccp_alpha=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'max_depth': [i for i in range(1, 20)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=hyperparameters,\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1  # Use all available CPU cores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy score on Validation set: \\n\")\n",
    "print(clf.best_score_ )\n",
    "print(\"---------------\")\n",
    "print(\"Best performing hyperparameters on Validation set: \")\n",
    "print(clf.best_params_)\n",
    "print(\"---------------\")\n",
    "print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = clf.best_estimator_\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, predictions, cmap=plt.cm.Greens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
