{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "\n",
    "# For reproducible results\n",
    "RANDOM_STATE_SEED = 732"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = pd.read_csv(\"processed_dataset_in_3.csv\")\n",
    "df_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# es realmente necesario volver a filtrar los datos si supuestamente el procesado no deveria tener valores infinitos\n",
    "\n",
    "print(np.any(np.isnan(df_dataset)))\n",
    "print(np.any(np.isinf(df_dataset)))\n",
    "\n",
    "# si trato de usar where infinite, normalmente trae malos resultados onda overflow de memoria\n",
    "df_dataset.isin([np.inf, -np.inf]).values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dataset.isinf()\n",
    "df_dataset.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_dataset.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# es realmente necesario volver a filtrar los datos si supuestamente el procesado no deveria tener valores infinitos\n",
    "\n",
    "print(np.any(np.isnan(df_dataset)))\n",
    "print(np.any(np.isinf(df_dataset)))\n",
    "\n",
    "# si trato de usar where infinite, normalmente trae malos resultados onda overflow de memoria\n",
    "df_dataset.isin([np.inf, -np.inf]).values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.describe()\n",
    "# df_label = np.array(df_dataset.pop('Label'))\n",
    "# df_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(df_dataset.pop('Label'))\n",
    "X = np.array(df_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X_scaler = MinMaxScaler().fit(X)\n",
    "pd.DataFrame(X_scaler.transform(X))\n",
    "X = np.array(X_scaler.transform(X))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = train_test_split(df_dataset, test_size=0.3, random_state=RANDOM_STATE_SEED)\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=RANDOM_STATE_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight  # For balanced class weighted classification training\n",
    "\n",
    "# Calculating class weights for balanced class weighted classifier training\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "print(class_weights)\n",
    "\n",
    "# Must be in dict format for scikitlearn\n",
    "class_weights = {\n",
    "    0: class_weights[0],\n",
    "    1: class_weights[1]\n",
    "}\n",
    "\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# predictions\n",
    "# joblib.dump(model, r\".\\trained_models\\remote-random-forest-classifier.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = joblib.load(f\".\\trained_models\\remote-random-forest-classifier\")\n",
    "# model = joblib.load(r\".\\trained_models\\remote-random-forest-classifier.pkl\")\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Step 7: Comparing Decision Tree, Random Forest, XGBoost, CatBoost, and LightGBM\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Bagging' : BaggingClassifier(),\n",
    "    'XGBoost': XGBClassifier(),\n",
    "    'CatBoost': CatBoostClassifier(),\n",
    "    'LightGBM': LGBMClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEMP\n",
    "classifiers.items()\n",
    "# for name, clf in classifiers.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate classifiers\n",
    "results = {}\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    results[name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Confusion Matrix': confusion_mat,\n",
    "        'Classification Report': class_report,\n",
    "        'ROC Curve': (fpr, tpr, roc_auc)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot for accuracy comparison\n",
    "accuracy_values = [result['Accuracy'] for result in results.values()]\n",
    "classifiers_names = list(classifiers.keys())\n",
    "\n",
    "plt.figure(figsize=(7, 3))\n",
    "plt.bar(classifiers_names, accuracy_values, color=['blue', 'green', 'red', 'purple', 'orange'])\n",
    "plt.xlabel('Classifiers')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Classifier Accuracy Comparison')\n",
    "plt.ylim([0, 1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices and classification reports\n",
    "for name, result in results.items():\n",
    "    print(f'\\n{name}:\\n')\n",
    "    # print(f'Confusion Matrix:\\n{result[\"Confusion Matrix\"]}\\n')\n",
    "    print(f'Classification Report:\\n{result[\"Classification Report\"]}\\n')\n",
    "\n",
    "    # Plot Confusion Matrix with Blues Colormap\n",
    "    plt.figure(figsize=(4, 2))\n",
    "    sns.heatmap(result[\"Confusion Matrix\"], annot=True, fmt='g', cmap=plt.cm.Greens, cbar=False)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix - {name}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Plot ROC curves\n",
    "plt.figure(figsize=(20, 6))\n",
    "for name, result in results.items():\n",
    "    fpr, tpr, roc_auc = result['ROC Curve']\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.9f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
